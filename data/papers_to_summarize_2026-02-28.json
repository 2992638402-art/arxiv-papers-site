{
  "date": "2026-02-28",
  "papers": [
    {
      "id": "2602.23363v1",
      "title": "MediX-R1: Open Ended Medical Reinforcement Learning",
      "summary": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com",
      "authors": [
        "Sahal Shaji Mullappilly",
        "Mohammed Irfan Kurpath",
        "Omair Mohamed",
        "Mohamed Zidan",
        "Fahad Khan",
        "Salman Khan",
        "Rao Anwer",
        "Hisham Cholakkal"
      ],
      "published": "2026-02-26T18:59:46Z",
      "updated": "2026-02-26T18:59:46Z",
      "categories": [
        "cs.CV"
      ],
      "pdfUrl": "http://arxiv.org/pdf/2602.23363v1.pdf",
      "category": "multimodal"
    },
    {
      "id": "2602.23351v1",
      "title": "Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning",
      "summary": "The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., \"at the game today!\" is a more likely caption than \"a photo of 37 people standing behind a field\". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.",
      "authors": [
        "Amita Kamath",
        "Jack Hessel",
        "Khyathi Chandu",
        "Jena D. Hwang",
        "Kai-Wei Chang",
        "Ranjay Krishna"
      ],
      "published": "2026-02-26T18:54:06Z",
      "updated": "2026-02-26T18:54:06Z",
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "pdfUrl": "http://arxiv.org/pdf/2602.23351v1.pdf",
      "category": "multimodal"
    },
    {
      "id": "2602.23253v1",
      "title": "SPARR: Simulation-based Policies with Asymmetric Real-world Residuals for Assembly",
      "summary": "Robotic assembly presents a long-standing challenge due to its requirement for precise, contact-rich manipulation. While simulation-based learning has enabled the development of robust assembly policies, their performance often degrades when deployed in real-world settings due to the sim-to-real gap. Conversely, real-world reinforcement learning (RL) methods avoid the sim-to-real gap, but rely heavily on human supervision and lack generalization ability to environmental changes. In this work, we propose a hybrid approach that combines a simulation-trained base policy with a real-world residual policy to efficiently adapt to real-world variations. The base policy, trained in simulation using low-level state observations and dense rewards, provides strong priors for initial behavior. The residual policy, learned in the real world using visual observations and sparse rewards, compensates for discrepancies in dynamics and sensor noise. Extensive real-world experiments demonstrate that our method, SPARR, achieves near-perfect success rates across diverse two-part assembly tasks. Compared to the state-of-the-art zero-shot sim-to-real methods, SPARR improves success rates by 38.4% while reducing cycle time by 29.7%. Moreover, SPARR requires no human expertise, in contrast to the state-of-the-art real-world RL approaches that depend heavily on human supervision.",
      "authors": [
        "Yijie Guo",
        "Iretiayo Akinola",
        "Lars Johannsmeier",
        "Hugo Hadfield",
        "Abhishek Gupta",
        "Yashraj Narang"
      ],
      "published": "2026-02-26T17:26:13Z",
      "updated": "2026-02-26T17:26:13Z",
      "categories": [
        "cs.RO"
      ],
      "pdfUrl": "http://arxiv.org/pdf/2602.23253v1.pdf",
      "category": "robotics"
    },
    {
      "id": "2602.23330v1",
      "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks",
      "summary": "The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.",
      "authors": [
        "Kunihiro Miyazaki",
        "Takanobu Kawahara",
        "Stephen Roberts",
        "Stefan Zohren"
      ],
      "published": "2026-02-26T18:37:36Z",
      "updated": "2026-02-26T18:37:36Z",
      "categories": [
        "cs.AI",
        "q-fin.TR"
      ],
      "pdfUrl": "http://arxiv.org/pdf/2602.23330v1.pdf",
      "category": "ml-theory"
    },
    {
      "id": "2602.23283v1",
      "title": "Simple Models, Real Swimming: Digital Twins for Tendon-Driven Underwater Robots",
      "summary": "Mimicking the graceful motion of swimming animals remains a core challenge in soft robotics due to the complexity of fluid-structure interaction and the difficulty of controlling soft, biomimetic bodies. Existing modeling approaches are often computationally expensive and impractical for complex control or reinforcement learning needed for realistic motions to emerge in robotic systems. In this work, we present a tendon-driven fish robot modeled in an efficient underwater swimmer environment using a simplified, stateless hydrodynamics formulation implemented in the widespread robotics framework MuJoCo. With just two real-world swimming trajectories, we identify five fluid parameters that allow a matching to experimental behavior and generalize across a range of actuation frequencies. We show that this stateless fluid model can generalize to unseen actuation and outperform classical analytical models such as the elongated body theory. This simulation environment runs faster than real-time and can easily enable downstream learning algorithms such as reinforcement learning for target tracking, reaching a 93% success rate. Due to the simplicity and ease of use of the model and our open-source simulation environment, our results show that even simple, stateless models -- when carefully matched to physical data -- can serve as effective digital twins for soft underwater robots, opening up new directions for scalable learning and control in aquatic environments.",
      "authors": [
        "Mike Y. Michelis",
        "Nana Obayashi",
        "Josie Hughes",
        "Robert K. Katzschmann"
      ],
      "published": "2026-02-26T17:55:22Z",
      "updated": "2026-02-26T17:55:22Z",
      "categories": [
        "cs.RO"
      ],
      "pdfUrl": "http://arxiv.org/pdf/2602.23283v1.pdf",
      "category": "robotics"
    }
  ]
}