{
  "date": "2026-02-28",
  "papers": [
    {
      "id": "2602.23330v1",
      "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks",
      "summary": "The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.",
      "authors": [
        "Kunihiro Miyazaki",
        "Takanobu Kawahara",
        "Stephen Roberts",
        "Stefan Zohren"
      ],
      "published": "2026-02-26T18:37:36Z",
      "updated": "2026-02-26T18:37:36Z",
      "categories": [
        "cs.AI",
        "q-fin.TR"
      ],
      "pdfUrl": "http://arxiv.org/pdf/2602.23330v1.pdf",
      "category": "ml-theory"
    },
    {
      "id": "2602.20229v1",
      "title": "HieraMAS: Optimizing Intra-Node LLM Mixtures and Inter-Node Topology for Multi-Agent Systems",
      "summary": "Multi-agent systems (MAS) built on large language models (LLMs) have shown strong performance across many tasks. Most existing approaches improve only one aspect at a time, such as the communication topology, role assignment, or LLM routing, while treating each agent as a single, indivisible unit. This misses the opportunity to use mixtures of LLMs within an agent to strengthen role-specific abilities. We propose HieraMAS, a hierarchical collaboration framework that combines intra-node LLM mixtures with an inter-node communication topology. HieraMAS introduces supernodes, where each functional role is implemented by multiple heterogeneous LLMs using a propose-synthesis structure. Optimizing HieraMAS creates unique credit-assignment challenges: final task performance depends heavily on the underlying LLMs' capabilities, which can lead reinforcement methods to incorrectly reward suboptimal configurations. To address this, we use a two-stage algorithm: (1) multi-level reward attribution, which provides fine-grained feedback at both the node level and the overall system level; (2) graph classification for topology selection, which treats choosing the communication structure as a holistic decision rather than optimizing edges one by one. Experiments on reasoning and coding benchmarks show that HieraMAS substantially outperforms existing methods while also delivering better cost-performance trade-offs.",
      "authors": [
        "Tianjun Yao",
        "Zhaoyi Li",
        "Zhiqiang Shen"
      ],
      "published": "2026-02-23T18:36:04Z",
      "updated": "2026-02-23T18:36:04Z",
      "categories": [
        "cs.MA"
      ],
      "pdfUrl": "http://arxiv.org/pdf/2602.20229v1.pdf",
      "category": "other"
    },
    {
      "id": "2602.21351v1",
      "title": "A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives",
      "summary": "The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.",
      "authors": [
        "Dmitrii Pantiukhin",
        "Ivan Kuznetsov",
        "Boris Shapkin",
        "Antonia Anna Jost",
        "Thomas Jung",
        "Nikolay Koldunov"
      ],
      "published": "2026-02-24T20:37:38Z",
      "updated": "2026-02-24T20:37:38Z",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.MA"
      ],
      "pdfUrl": "http://arxiv.org/pdf/2602.21351v1.pdf",
      "category": "ml-theory"
    },
    {
      "id": "2602.23259v1",
      "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving",
      "summary": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.",
      "authors": [
        "Jiangxin Sun",
        "Feng Xue",
        "Teng Long",
        "Chang Liu",
        "Jian-Fang Hu",
        "Wei-Shi Zheng",
        "Nicu Sebe"
      ],
      "published": "2026-02-26T17:32:30Z",
      "updated": "2026-02-26T17:32:30Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "pdfUrl": "http://arxiv.org/pdf/2602.23259v1.pdf",
      "category": "robotics"
    },
    {
      "id": "2602.22786v1",
      "title": "QSIM: Mitigating Overestimation in Multi-Agent Reinforcement Learning via Action Similarity Weighted Q-Learning",
      "summary": "Value decomposition (VD) methods have achieved remarkable success in cooperative multi-agent reinforcement learning (MARL). However, their reliance on the max operator for temporal-difference (TD) target calculation leads to systematic Q-value overestimation. This issue is particularly severe in MARL due to the combinatorial explosion of the joint action space, which often results in unstable learning and suboptimal policies. To address this problem, we propose QSIM, a similarity weighted Q-learning framework that reconstructs the TD target using action similarity. Instead of using the greedy joint action directly, QSIM forms a similarity weighted expectation over a structured near-greedy joint action space. This formulation allows the target to integrate Q-values from diverse yet behaviorally related actions while assigning greater influence to those that are more similar to the greedy choice. By smoothing the target with structurally relevant alternatives, QSIM effectively mitigates overestimation and improves learning stability. Extensive experiments demonstrate that QSIM can be seamlessly integrated with various VD methods, consistently yielding superior performance and stability compared to the original algorithms. Furthermore, empirical analysis confirms that QSIM significantly mitigates the systematic value overestimation in MARL. Code is available at https://github.com/MaoMaoLYJ/pymarl-qsim.",
      "authors": [
        "Yuanjun Li",
        "Bin Zhang",
        "Hao Chen",
        "Zhouyang Jiang",
        "Dapeng Li",
        "Zhiwei Xu"
      ],
      "published": "2026-02-26T09:20:46Z",
      "updated": "2026-02-26T09:20:46Z",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "http://arxiv.org/pdf/2602.22786v1.pdf",
      "category": "ml-theory"
    }
  ],
  "ids": [
    "2602.23330v1",
    "2602.20229v1",
    "2602.21351v1",
    "2602.23259v1",
    "2602.22786v1"
  ]
}