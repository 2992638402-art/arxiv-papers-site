# Flexi-NeurA: A Configurable Neuromorphic Accelerator with Adaptive Bit-Precision Exploration for Edge SNNs

> **arXiv ID:** 2602.18140
> **åˆ†ç±»:** cs.AR, cs.NE
> **å‘å¸ƒæ—¶é—´:** 2026-02-20

## ğŸ“„ è®ºæ–‡ä¿¡æ¯

- **ä½œè€…:** Mohammad Farahani, Mohammad Rasoul Roshanshah, Saeed Safari
- **PDF:** [ä¸‹è½½](https://arxiv.org/pdf/2602.18140)
- **arXiv é“¾æ¥:** [æŸ¥çœ‹](https://arxiv.org/abs/2602.18140)

## ğŸ“ æ‘˜è¦

Neuromorphic accelerators promise unparalleled energy efficiency and computational density for spiking neural networks (SNNs), especially in edge intelligence applications. However, most existing platforms exhibit rigid architectures with limited configurability, restricting their adaptability to heterogeneous workloads and diverse design objectives. To address these limitations, we present Flexi-NeurA -- a parameterizable neuromorphic accelerator (core) that unifies configurability, flexibility, and efficiency. Flexi-NeurA allows users to customize neuron models, network structures, and precision settings at design time. By pairing these design-time configurability and flexibility features with a time-multiplexed and event-driven processing approach, Flexi-NeurA substantially reduces the required hardware resources and total power while preserving high efficiency and low inference latency. Complementing this, we introduce Flex-plorer, a heuristic-guided design-space exploration (DSE) tool that determines cost-effective fixed-point precisions for critical parameters -- such as decay factors, synaptic weights, and membrane potentials -- based on user-defined trade-offs between accuracy and resource usage. Based on the configuration selected through the Flex-plorer process, RTL code is configured to match the specified design. Comprehensive evaluations across MNIST, SHD, and DVS benchmarks demonstrate that the Flexi-NeurA and Flex-plorer co-framework achieves substantial improvements in accuracy, latency, and energy efficiency. A three-layer 256--128--10 fully connected network with LIF neurons mapped onto two processing cores achieves 97.23% accuracy on MNIST with 1.1~ms inference latency, utilizing only 1,623 logic cells, 7 BRAMs, and 111~mW of total power -- establishing Flexi-NeurA as a scalable, edge-ready neuromorphic platform.

## ğŸ·ï¸ æ ‡ç­¾

`AI` `æœºå™¨å­¦ä¹ `

---

**æ³¨æ„ï¼š** æœ¬æ€»ç»“åŸºäº arXiv åŸå§‹æ‘˜è¦ã€‚AI æ·±åº¦è§£è¯»æ­£åœ¨ç”Ÿæˆä¸­...

