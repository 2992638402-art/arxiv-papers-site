# Linear Reservoir: A Diagonalization-Based Optimization

> **arXiv ID:** 2602.19802
> **åˆ†ç±»:** cs.DC, cs.NE, math.CV, math.DS
> **å‘å¸ƒæ—¶é—´:** 2026-02-23

## ğŸ“„ è®ºæ–‡ä¿¡æ¯

- **ä½œè€…:** Romain de Coudenhove, Yannis Bendi-Ouis, Anthony Strock, Xavier Hinaut
- **PDF:** [ä¸‹è½½](https://arxiv.org/pdf/2602.19802)
- **arXiv é“¾æ¥:** [æŸ¥çœ‹](https://arxiv.org/abs/2602.19802)

## ğŸ“ æ‘˜è¦

We introduce a diagonalization-based optimization for Linear Echo State Networks (ESNs) that reduces the per-step computational complexity of reservoir state updates from O(N^2) to O(N). By reformulating reservoir dynamics in the eigenbasis of the recurrent matrix, the recurrent update becomes a set of independent element-wise operations, eliminating the matrix multiplication. We further propose three methods to use our optimization depending on the situation: (i) Eigenbasis Weight Transformation (EWT), which preserves the dynamics of standard and trained Linear ESNs, (ii) End-to-End Eigenbasis Training (EET), which directly optimizes readout weights in the transformed space and (iii) Direct Parameter Generation (DPG), that bypasses matrix diagonalization by directly sampling eigenvalues and eigenvectors, achieving comparable performance than standard Linear ESNs. Across all experiments, both our methods preserve predictive accuracy while offering significant computational speedups, making them a replacement of standard Linear ESNs computations and training, and suggesting a shift of paradigm in linear ESN towards the direct selection of eigenvalues.

## ğŸ·ï¸ æ ‡ç­¾

`AI` `æœºå™¨å­¦ä¹ `

---

**æ³¨æ„ï¼š** æœ¬æ€»ç»“åŸºäº arXiv åŸå§‹æ‘˜è¦ã€‚AI æ·±åº¦è§£è¯»æ­£åœ¨ç”Ÿæˆä¸­...

