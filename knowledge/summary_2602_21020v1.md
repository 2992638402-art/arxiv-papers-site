# Matching Multiple Experts: On the Exploitability of Multi-Agent Imitation Learning

> **arXiv ID:** 2602.21020
> **åˆ†ç±»:** cs.LG, cs.GT, cs.MA
> **å‘å¸ƒæ—¶é—´:** 2026-02-24

## ğŸ“„ è®ºæ–‡ä¿¡æ¯

- **ä½œè€…:** Antoine Bergerault, Volkan Cevher, Negar Mehr
- **PDF:** [ä¸‹è½½](https://arxiv.org/pdf/2602.21020)
- **arXiv é“¾æ¥:** [æŸ¥çœ‹](https://arxiv.org/abs/2602.21020)

## ğŸ“ æ‘˜è¦

Multi-agent imitation learning (MA-IL) aims to learn optimal policies from expert demonstrations of interactions in multi-agent interactive domains. Despite existing guarantees on the performance of the resulting learned policies, characterizations of how far the learned polices are from a Nash equilibrium are missing for offline MA-IL. In this paper, we demonstrate impossibility and hardness results of learning low-exploitable policies in general $n$-player Markov Games. We do so by providing examples where even exact measure matching fails, and demonstrating a new hardness result on characterizing the Nash gap given a fixed measure matching error. We then show how these challenges can be overcome using strategic dominance assumptions on the expert equilibrium. Specifically, for the case of dominant strategy expert equilibria, assuming Behavioral Cloning error $Îµ_{\text{BC}}$, this provides a Nash imitation gap of $\mathcal{O}\left(nÎµ_{\text{BC}}/(1-Î³)^2\right)$ for a discount factor $Î³$. We generalize this result with a new notion of best-response continuity, and argue that this is implicitly encouraged by standard regularization techniques.

## ğŸ·ï¸ æ ‡ç­¾

`æœºå™¨å­¦ä¹ ` `ç†è®º` `ä¼˜åŒ–`

---

**æ³¨æ„ï¼š** æœ¬æ€»ç»“åŸºäº arXiv åŸå§‹æ‘˜è¦ã€‚AI æ·±åº¦è§£è¯»æ­£åœ¨ç”Ÿæˆä¸­...

