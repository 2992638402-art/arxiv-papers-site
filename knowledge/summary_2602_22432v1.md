# LoBoost: Fast Model-Native Local Conformal Prediction for Gradient-Boosted Trees

> **arXiv ID:** 2602.22432
> **åˆ†ç±»:** stat.ML, cs.LG
> **å‘å¸ƒæ—¶é—´:** 2026-02-25

## ğŸ“„ è®ºæ–‡ä¿¡æ¯

- **ä½œè€…:** Vagner Santos, Victor Coscrato, Luben Cabezas, Rafael Izbicki, Thiago Ramos
- **PDF:** [ä¸‹è½½](https://arxiv.org/pdf/2602.22432)
- **arXiv é“¾æ¥:** [æŸ¥çœ‹](https://arxiv.org/abs/2602.22432)

## ğŸ“ æ‘˜è¦

Gradient-boosted decision trees are among the strongest off-the-shelf predictors for tabular regression, but point predictions alone do not quantify uncertainty. Conformal prediction provides distribution-free marginal coverage, yet split conformal uses a single global residual quantile and can be poorly adaptive under heteroscedasticity. Methods that improve adaptivity typically fit auxiliary nuisance models or introduce additional data splits/partitions to learn the conformal score, increasing cost and reducing data efficiency. We propose LoBoost, a model-native local conformal method that reuses the fitted ensemble's leaf structure to define multiscale calibration groups. Each input is encoded by its sequence of visited leaves; at resolution level k, we group points by matching prefixes of leaf indices across the first k trees and calibrate residual quantiles within each group. LoBoost requires no retraining, auxiliary models, or extra splitting beyond the standard train/calibration split. Experiments show competitive interval quality, improved test MSE on most datasets, and large calibration speedups.

## ğŸ·ï¸ æ ‡ç­¾

`æœºå™¨å­¦ä¹ ` `ç†è®º` `ä¼˜åŒ–`

---

**æ³¨æ„ï¼š** æœ¬æ€»ç»“åŸºäº arXiv åŸå§‹æ‘˜è¦ã€‚AI æ·±åº¦è§£è¯»æ­£åœ¨ç”Ÿæˆä¸­...

