# InCoM: Intent-Driven Perception and Structured Coordination for Whole-Body Mobile Manipulation

> **arXiv ID:** 2602.23024
> **åˆ†ç±»:** cs.RO
> **å‘å¸ƒæ—¶é—´:** 2026-02-26

## ğŸ“„ è®ºæ–‡ä¿¡æ¯

- **ä½œè€…:** Jiahao Liu, Cui Wenbo, Haoran Li, Dongbin Zhao
- **PDF:** [ä¸‹è½½](https://arxiv.org/pdf/2602.23024)
- **arXiv é“¾æ¥:** [æŸ¥çœ‹](https://arxiv.org/abs/2602.23024)

## ğŸ“ æ‘˜è¦

Whole-body mobile manipulation is a fundamental capability for general-purpose robotic agents, requiring both coordinated control of the mobile base and manipulator and robust perception under dynamically changing viewpoints. However, existing approaches face two key challenges: strong coupling between base and arm actions complicates whole-body control optimization, and perceptual attention is often poorly allocated as viewpoints shift during mobile manipulation. We propose InCoM, an intent-driven perception and structured coordination framework for whole-body mobile manipulation. InCoM infers latent motion intent to dynamically reweight multi-scale perceptual features, enabling stage-adaptive allocation of perceptual attention. To support robust cross-modal perception, InCoM further incorporates a geometric-semantic structured alignment mechanism that enhances multimodal correspondence. On the control side, we design a decoupled coordinated flow matching action decoder that explicitly models coordinated base-arm action generation, alleviating optimization difficulties caused by control coupling. Without access to privileged perceptual information, InCoM outperforms state-of-the-art methods on three ManiSkill-HAB scenarios by 28.2%, 26.1%, and 23.6% in success rate, demonstrating strong effectiveness for whole-body mobile manipulation.

## ğŸ·ï¸ æ ‡ç­¾

`æœºå™¨äºº` `æ§åˆ¶` `å…·èº«æ™ºèƒ½`

---

**æ³¨æ„ï¼š** æœ¬æ€»ç»“åŸºäº arXiv åŸå§‹æ‘˜è¦ã€‚AI æ·±åº¦è§£è¯»æ­£åœ¨ç”Ÿæˆä¸­...

