# Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction

> **arXiv ID:** 2602.23312
> **åˆ†ç±»:** cs.HC, cs.AI, cs.LG, cs.RO, eess.SY
> **å‘å¸ƒæ—¶é—´:** 2026-02-26

## ğŸ“„ è®ºæ–‡ä¿¡æ¯

- **ä½œè€…:** Rafael R. Baptista, AndrÃ© de Lima Salgado, Ricardo V. Godoy, Marcelo Becker, Thiago Boaventura, Gustavo J. G. Lahr
- **PDF:** [ä¸‹è½½](https://arxiv.org/pdf/2602.23312)
- **arXiv é“¾æ¥:** [æŸ¥çœ‹](https://arxiv.org/abs/2602.23312)

## ğŸ“ æ‘˜è¦

Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots. While large language models (LLMs) have shown promise for natural communication, their size and latency limit on-device deployment. Small language models (SLMs) offer a potential alternative, but their effectiveness for role classification in HRI has not been systematically evaluated. In this paper, we present a benchmark of SLMs for leader-follower communication, introducing a novel dataset derived from a published database and augmented with synthetic samples to capture interaction-specific dynamics. We investigate two adaptation strategies: prompt engineering and fine-tuning, studied under zero-shot and one-shot interaction modes, compared with an untrained baseline. Experiments with Qwen2.5-0.5B reveal that zero-shot fine-tuning achieves robust classification performance (86.66% accuracy) while maintaining low latency (22.2 ms per sample), significantly outperforming baseline and prompt-engineered approaches. However, results also indicate a performance degradation in one-shot modes, where increased context length challenges the model's architectural capacity. These findings demonstrate that fine-tuned SLMs provide an effective solution for direct role assignment, while highlighting critical trade-offs between dialogue complexity and classification reliability on the edge.

## ğŸ·ï¸ æ ‡ç­¾

`æœºå™¨äºº` `æ§åˆ¶` `å…·èº«æ™ºèƒ½`

---

**æ³¨æ„ï¼š** æœ¬æ€»ç»“åŸºäº arXiv åŸå§‹æ‘˜è¦ã€‚AI æ·±åº¦è§£è¯»æ­£åœ¨ç”Ÿæˆä¸­...

