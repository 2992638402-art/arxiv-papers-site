# MediX-R1: Open Ended Medical Reinforcement Learning

> **arXiv ID:** 2602.23363
> **åˆ†ç±»:** cs.CV
> **å‘å¸ƒæ—¶é—´:** 2026-02-26

## ğŸ“„ è®ºæ–‡ä¿¡æ¯

- **ä½œè€…:** Sahal Shaji Mullappilly, Mohammed Irfan Kurpath, Omair Mohamed, Mohamed Zidan, Fahad Khan, Salman Khan, Rao Anwer, Hisham Cholakkal
- **PDF:** [ä¸‹è½½](https://arxiv.org/pdf/2602.23363)
- **arXiv é“¾æ¥:** [æŸ¥çœ‹](https://arxiv.org/abs/2602.23363)

## ğŸ“ æ‘˜è¦

We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com

## ğŸ·ï¸ æ ‡ç­¾

`å¤šæ¨¡æ€` `è§†è§‰è¯­è¨€` `VLM`

---

**æ³¨æ„ï¼š** æœ¬æ€»ç»“åŸºäº arXiv åŸå§‹æ‘˜è¦ã€‚AI æ·±åº¦è§£è¯»æ­£åœ¨ç”Ÿæˆä¸­...

