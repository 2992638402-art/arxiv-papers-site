<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Fine-Pruning: A Biologically Inspired Algorithm for Personalization of Machine Learning Models - arXiv 2602.18507 è®ºæ–‡æ€»ç»“">
    <title>Fine-Pruning: A Biologically Inspired Algorithm for Personalization of Machine Learning Models | arXiv AI è®ºæ–‡æ¯æ—¥ç²¾é€‰</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ğŸ“š arXiv AI è®ºæ–‡æ¯æ—¥ç²¾é€‰</h1>
            <p class="subtitle">ç²¾é€‰ AI/ML å‰æ²¿è®ºæ–‡ï¼Œæ·±åº¦è§£è¯»ï¼ŒåŠ©ä½ æŠŠæ¡ç ”ç©¶åŠ¨æ€</p>
        </div>
    </header>

    <nav>
        <ul>
            <li><a href="../index.html">ğŸ  é¦–é¡µ</a></li>
            <li><a href="../daily/2026-02-28.html">ğŸ“… æ¯æ—¥æ€»ç»“</a></li>
            <li><a href="#categories">ğŸ·ï¸ åˆ†ç±»æµè§ˆ</a></li>
            <li><a href="https://github.com" target="_blank">ğŸ’» GitHub</a></li>
        </ul>
    </nav>

    <main>
        <div class="paper-content">
            <h1>Fine-Pruning: A Biologically Inspired Algorithm for Personalization of Machine Learning Models</h1>
<blockquote>
<p><strong>arXiv ID:</strong> 2602.18507<br><strong>åˆ†ç±»:</strong> cs.NE, q-bio.NC<br><strong>å‘å¸ƒæ—¶é—´:</strong> 2026-02-18</p>
</blockquote>
<h2>ğŸ“„ è®ºæ–‡ä¿¡æ¯</h2>
<ul>
<li><strong>ä½œè€…:</strong> Joseph Bingham, Saman Zonouz, Dvir Aran</li>
<li><strong>PDF:</strong> <a href="https://arxiv.org/pdf/2602.18507">ä¸‹è½½</a></li>
<li><strong>arXiv é“¾æ¥:</strong> <a href="https://arxiv.org/abs/2602.18507">æŸ¥çœ‹</a></li>
</ul>
<h2>ğŸ“ æ‘˜è¦</h2>
<p>Neural networks have long strived to emulate the learning capabilities of the human brain. While deep neural networks (DNNs) draw inspiration from the brain in neuron design, their training methods diverge from biological foundations. Backpropagation, the primary training method for DNNs, requires substantial computational resources and fully labeled datasets, presenting major bottlenecks in development and application. This work demonstrates that by returning to biomimicry, specifically mimicking how the brain learns through pruning, we can solve various classical machine learning problems while utilizing orders of magnitude fewer computational resources and no labels. Our experiments successfully personalized multiple speech recognition and image classification models, including ResNet50 on ImageNet, resulting in increased sparsity of approximately 70% while simultaneously improving model accuracy to around 90%, all without the limitations of backpropagation. This biologically inspired approach offers a promising avenue for efficient, personalized machine learning models in resource-constrained environments.</p>
<h2>ğŸ·ï¸ æ ‡ç­¾</h2>
<p><code>AI</code> <code>æœºå™¨å­¦ä¹ </code></p>
<hr>
<p><strong>æ³¨æ„ï¼š</strong> æœ¬æ€»ç»“åŸºäº arXiv åŸå§‹æ‘˜è¦ã€‚AI æ·±åº¦è§£è¯»æ­£åœ¨ç”Ÿæˆä¸­...</p>

        </div>

        <div style="margin-top: 2rem; text-align: center;">
            <a href="../index.html" class="btn btn-secondary">â† è¿”å›é¦–é¡µ</a>
            <a href="https://arxiv.org/abs/2602.18507" class="btn" target="_blank">ğŸ“„ æŸ¥çœ‹ arXiv åŸæ–‡</a>
        </div>
    </main>

    <footer>
        <p>Â© 2026 arXiv AI è®ºæ–‡æ¯æ—¥ç²¾é€‰ | æ•°æ®æ¥æºï¼š<a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>