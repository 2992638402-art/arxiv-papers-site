<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="HieraMAS: Optimizing Intra-Node LLM Mixtures and Inter-Node Topology for Multi-Agent Systems - arXiv 2602.20229 è®ºæ–‡æ€»ç»“">
    <title>HieraMAS: Optimizing Intra-Node LLM Mixtures and Inter-Node Topology for Multi-Agent Systems | arXiv AI è®ºæ–‡æ¯æ—¥ç²¾é€‰</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ğŸ“š arXiv AI è®ºæ–‡æ¯æ—¥ç²¾é€‰</h1>
            <p class="subtitle">ç²¾é€‰ AI/ML å‰æ²¿è®ºæ–‡ï¼Œæ·±åº¦è§£è¯»ï¼ŒåŠ©ä½ æŠŠæ¡ç ”ç©¶åŠ¨æ€</p>
        </div>
    </header>

    <nav>
        <ul>
            <li><a href="../index.html">ğŸ  é¦–é¡µ</a></li>
            <li><a href="../daily/2026-02-28.html">ğŸ“… æ¯æ—¥æ€»ç»“</a></li>
            <li><a href="#categories">ğŸ·ï¸ åˆ†ç±»æµè§ˆ</a></li>
            <li><a href="https://github.com" target="_blank">ğŸ’» GitHub</a></li>
        </ul>
    </nav>

    <main>
        <div class="paper-content">
            <h1>HieraMAS: Optimizing Intra-Node LLM Mixtures and Inter-Node Topology for Multi-Agent Systems</h1>
<blockquote>
<p><strong>arXiv ID:</strong> 2602.20229<br><strong>åˆ†ç±»:</strong> cs.MA<br><strong>å‘å¸ƒæ—¶é—´:</strong> 2026-02-23</p>
</blockquote>
<h2>ğŸ“„ è®ºæ–‡ä¿¡æ¯</h2>
<ul>
<li><strong>ä½œè€…:</strong> Tianjun Yao, Zhaoyi Li, Zhiqiang Shen</li>
<li><strong>PDF:</strong> <a href="https://arxiv.org/pdf/2602.20229">ä¸‹è½½</a></li>
<li><strong>arXiv é“¾æ¥:</strong> <a href="https://arxiv.org/abs/2602.20229">æŸ¥çœ‹</a></li>
</ul>
<h2>ğŸ“ æ‘˜è¦</h2>
<p>Multi-agent systems (MAS) built on large language models (LLMs) have shown strong performance across many tasks. Most existing approaches improve only one aspect at a time, such as the communication topology, role assignment, or LLM routing, while treating each agent as a single, indivisible unit. This misses the opportunity to use mixtures of LLMs within an agent to strengthen role-specific abilities. We propose HieraMAS, a hierarchical collaboration framework that combines intra-node LLM mixtures with an inter-node communication topology. HieraMAS introduces supernodes, where each functional role is implemented by multiple heterogeneous LLMs using a propose-synthesis structure. Optimizing HieraMAS creates unique credit-assignment challenges: final task performance depends heavily on the underlying LLMs&#39; capabilities, which can lead reinforcement methods to incorrectly reward suboptimal configurations. To address this, we use a two-stage algorithm: (1) multi-level reward attribution, which provides fine-grained feedback at both the node level and the overall system level; (2) graph classification for topology selection, which treats choosing the communication structure as a holistic decision rather than optimizing edges one by one. Experiments on reasoning and coding benchmarks show that HieraMAS substantially outperforms existing methods while also delivering better cost-performance trade-offs.</p>
<h2>ğŸ·ï¸ æ ‡ç­¾</h2>
<p><code>AI</code> <code>æœºå™¨å­¦ä¹ </code></p>
<hr>
<p><strong>æ³¨æ„ï¼š</strong> æœ¬æ€»ç»“åŸºäº arXiv åŸå§‹æ‘˜è¦ã€‚AI æ·±åº¦è§£è¯»æ­£åœ¨ç”Ÿæˆä¸­...</p>

        </div>

        <div style="margin-top: 2rem; text-align: center;">
            <a href="../index.html" class="btn btn-secondary">â† è¿”å›é¦–é¡µ</a>
            <a href="https://arxiv.org/abs/2602.20229" class="btn" target="_blank">ğŸ“„ æŸ¥çœ‹ arXiv åŸæ–‡</a>
        </div>
    </main>

    <footer>
        <p>Â© 2026 arXiv AI è®ºæ–‡æ¯æ—¥ç²¾é€‰ | æ•°æ®æ¥æºï¼š<a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>