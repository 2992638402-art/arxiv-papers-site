<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="InCoM: Intent-Driven Perception and Structured Coordination for Whole-Body Mobile Manipulation - arXiv 2602.23024 è®ºæ–‡æ€»ç»“">
    <title>InCoM: Intent-Driven Perception and Structured Coordination for Whole-Body Mobile Manipulation | arXiv AI è®ºæ–‡æ¯æ—¥ç²¾é€‰</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ğŸ“š arXiv AI è®ºæ–‡æ¯æ—¥ç²¾é€‰</h1>
            <p class="subtitle">ç²¾é€‰ AI/ML å‰æ²¿è®ºæ–‡ï¼Œæ·±åº¦è§£è¯»ï¼ŒåŠ©ä½ æŠŠæ¡ç ”ç©¶åŠ¨æ€</p>
        </div>
    </header>

    <nav>
        <ul>
            <li><a href="../index.html">ğŸ  é¦–é¡µ</a></li>
            <li><a href="../daily/2026-02-28.html">ğŸ“… æ¯æ—¥æ€»ç»“</a></li>
            <li><a href="#categories">ğŸ·ï¸ åˆ†ç±»æµè§ˆ</a></li>
            <li><a href="https://github.com" target="_blank">ğŸ’» GitHub</a></li>
        </ul>
    </nav>

    <main>
        <div class="paper-content">
            <h1>InCoM: Intent-Driven Perception and Structured Coordination for Whole-Body Mobile Manipulation</h1>
<blockquote>
<p><strong>arXiv ID:</strong> 2602.23024<br><strong>åˆ†ç±»:</strong> cs.RO<br><strong>å‘å¸ƒæ—¶é—´:</strong> 2026-02-26</p>
</blockquote>
<h2>ğŸ“„ è®ºæ–‡ä¿¡æ¯</h2>
<ul>
<li><strong>ä½œè€…:</strong> Jiahao Liu, Cui Wenbo, Haoran Li, Dongbin Zhao</li>
<li><strong>PDF:</strong> <a href="https://arxiv.org/pdf/2602.23024">ä¸‹è½½</a></li>
<li><strong>arXiv é“¾æ¥:</strong> <a href="https://arxiv.org/abs/2602.23024">æŸ¥çœ‹</a></li>
</ul>
<h2>ğŸ“ æ‘˜è¦</h2>
<p>Whole-body mobile manipulation is a fundamental capability for general-purpose robotic agents, requiring both coordinated control of the mobile base and manipulator and robust perception under dynamically changing viewpoints. However, existing approaches face two key challenges: strong coupling between base and arm actions complicates whole-body control optimization, and perceptual attention is often poorly allocated as viewpoints shift during mobile manipulation. We propose InCoM, an intent-driven perception and structured coordination framework for whole-body mobile manipulation. InCoM infers latent motion intent to dynamically reweight multi-scale perceptual features, enabling stage-adaptive allocation of perceptual attention. To support robust cross-modal perception, InCoM further incorporates a geometric-semantic structured alignment mechanism that enhances multimodal correspondence. On the control side, we design a decoupled coordinated flow matching action decoder that explicitly models coordinated base-arm action generation, alleviating optimization difficulties caused by control coupling. Without access to privileged perceptual information, InCoM outperforms state-of-the-art methods on three ManiSkill-HAB scenarios by 28.2%, 26.1%, and 23.6% in success rate, demonstrating strong effectiveness for whole-body mobile manipulation.</p>
<h2>ğŸ·ï¸ æ ‡ç­¾</h2>
<p><code>æœºå™¨äºº</code> <code>æ§åˆ¶</code> <code>å…·èº«æ™ºèƒ½</code></p>
<hr>
<p><strong>æ³¨æ„ï¼š</strong> æœ¬æ€»ç»“åŸºäº arXiv åŸå§‹æ‘˜è¦ã€‚AI æ·±åº¦è§£è¯»æ­£åœ¨ç”Ÿæˆä¸­...</p>

        </div>

        <div style="margin-top: 2rem; text-align: center;">
            <a href="../index.html" class="btn btn-secondary">â† è¿”å›é¦–é¡µ</a>
            <a href="https://arxiv.org/abs/2602.23024" class="btn" target="_blank">ğŸ“„ æŸ¥çœ‹ arXiv åŸæ–‡</a>
        </div>
    </main>

    <footer>
        <p>Â© 2026 arXiv AI è®ºæ–‡æ¯æ—¥ç²¾é€‰ | æ•°æ®æ¥æºï¼š<a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>